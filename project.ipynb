{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe208b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce5f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting a subset of 20,000 images to train, calibrate and test\n",
    "# Shuffling the dataset to remove patterns/groups of related images\n",
    "import random\n",
    "random.seed(10) # comment out later\n",
    "img_index = [i for i in range(20000)]\n",
    "random.shuffle(img_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d21ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing attributes Smiling, Male, Young.. for each image\n",
    "import pandas as pd\n",
    "df = pd.read_csv('list_attr_celeba.txt', skiprows=1, header=0, sep='\\s+')\n",
    "\n",
    "img_df = df[[\"Smiling\", \"Male\", \"Young\", \"Blond_Hair\"]].head(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6f5946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction with Vision Transformer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ViT Embeddings: 100%|██████████| 20000/20000 [4:46:26<00:00,  1.16it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (20000, 768)\n",
      "Embeddings saved to ’embeddings.npy’.\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "def extract_features():\n",
    "    \"\"\"\n",
    "    Extracts features from images using a pre-trained Vision Transformer (ViT)\n",
    "    and saves them to a file.\n",
    "    \"\"\"\n",
    "    EMBEDDINGS_FILE = 'embeddings.npy'\n",
    "    IMAGE_DIR = 'celeba_selection'\n",
    "    NUM_IMAGES_TO_PROCESS = 20000\n",
    "    random.seed(10) # comment out later, for reproducibility\n",
    "\n",
    "    if os.path.exists(EMBEDDINGS_FILE):\n",
    "        print(f\"Embeddings file ’{EMBEDDINGS_FILE}’ already exists. Skipping feature extraction.\")\n",
    "        return\n",
    "    print(\"Starting feature extraction with Vision Transformer...\")\n",
    "\n",
    "    # 1. Load pre-trained Vision Transformer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vit = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1).to(device)\n",
    "    vit.eval() # Set model to evaluation mode\n",
    "\n",
    "    # 2. Define preprocessing steps consistent with ImageNet training\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # 3. Get the list of images to process\n",
    "    # sorted\n",
    "    # image_list = sorted(os.listdir(IMAGE_DIR))[:NUM_IMAGES_TO_PROCESS]\n",
    "    # shuffle\n",
    "    image_list = (os.listdir(IMAGE_DIR))[:NUM_IMAGES_TO_PROCESS]\n",
    "    random.shuffle(image_list)\n",
    "    all_features = []\n",
    "\n",
    "    # 4. Extract embeddings for each image\n",
    "    with torch.no_grad():\n",
    "        for fname in tqdm(image_list, desc=\"Extracting ViT Embeddings\"):\n",
    "            img = Image.open(os.path.join(IMAGE_DIR, fname)).convert(\"RGB\")\n",
    "            x = preprocess(img).unsqueeze(0).to(device)\n",
    "            # Manually replicate the forward pass to get the features before the\n",
    "            # classification head, as the internal API (like .process_input)\n",
    "            # can change.\n",
    "            # 1. Process input using the private _process_input method\n",
    "            x_processed = vit._process_input(x)\n",
    "            n = x_processed.shape[0]\n",
    "            # 2. Add the class token\n",
    "            batch_class_token = vit.class_token.expand(n, -1, -1)\n",
    "            x_with_token = torch.cat([batch_class_token, x_processed], dim=1)\n",
    "            # 3. Pass through the encoder\n",
    "            encoded_features = vit.encoder(x_with_token)\n",
    "            # 4. Get the class token’s output (this is the feature vector)\n",
    "            features = encoded_features[:, 0]\n",
    "            all_features.append(features.cpu().numpy().flatten())\n",
    "    all_features_np = np.array(all_features)\n",
    "    print(f\"Feature matrix shape: {all_features_np.shape}\")\n",
    "    \n",
    "    # 5. Save embeddings for later use\n",
    "    np.save(EMBEDDINGS_FILE, all_features_np)\n",
    "    print(f\"Embeddings saved to ’{EMBEDDINGS_FILE}’.\")\n",
    "\n",
    "extract_features()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c92f295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(20000, 768)\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.load(\"embeddings.npy\")\n",
    "print(type(embeddings))\n",
    "print(embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernel