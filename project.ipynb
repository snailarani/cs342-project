{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8fe208b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0ce5f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting a subset of 20,000 images to train, calibrate and test\n",
    "# Shuffling the dataset to remove patterns/groups of related images\n",
    "import random\n",
    "random.seed(10) # comment out later\n",
    "img_index = [i for i in range(20000)]\n",
    "random.shuffle(img_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "08d21ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Smiling  Male  Young  Blond_Hair\n",
      "000011.jpg        1    -1      1          -1\n",
      "000012.jpg        1     1      1          -1\n",
      "000013.jpg        1     1      1           1\n",
      "000014.jpg        1    -1      1          -1\n",
      "000015.jpg       -1     1     -1          -1\n"
     ]
    }
   ],
   "source": [
    "# Storing attributes Smiling, Male, Young.. for each image\n",
    "import pandas as pd\n",
    "df = pd.read_csv('list_attr_celeba.txt', skiprows=1, header=0, sep='\\s+')\n",
    "\n",
    "img_df = df[[\"Smiling\", \"Male\", \"Young\", \"Blond_Hair\"]].head(20000)\n",
    "print(img_df.iloc[10:15, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6c6f5946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings file ’embeddings.npy’ already exists. Skipping feature extraction.\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "def extract_features():\n",
    "    \"\"\"\n",
    "    Extracts features from images using a pre-trained Vision Transformer (ViT)\n",
    "    and saves them to a file.\n",
    "    \"\"\"\n",
    "    EMBEDDINGS_FILE = 'embeddings.npy'\n",
    "    IMAGE_DIR = 'celeba_selection'\n",
    "    NUM_IMAGES_TO_PROCESS = 20000 #later change to SAMPLE_SIZE\n",
    "    random.seed(10) # comment out later, for reproducibility\n",
    "\n",
    "    if os.path.exists(EMBEDDINGS_FILE):\n",
    "        print(f\"Embeddings file ’{EMBEDDINGS_FILE}’ already exists. Skipping feature extraction.\")\n",
    "        return\n",
    "    print(\"Starting feature extraction with Vision Transformer...\")\n",
    "\n",
    "    # 1. Load pre-trained Vision Transformer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    vit = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1).to(device)\n",
    "    vit.eval() # Set model to evaluation mode\n",
    "\n",
    "    # 2. Define preprocessing steps consistent with ImageNet training\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # 3. Get the list of images to process\n",
    "\n",
    "    image_list = sorted(os.listdir(IMAGE_DIR))[:NUM_IMAGES_TO_PROCESS]\n",
    "    all_features = []\n",
    "\n",
    "    # 4. Extract embeddings for each image\n",
    "    with torch.no_grad():\n",
    "        for fname in tqdm(image_list, desc=\"Extracting ViT Embeddings\"):\n",
    "            img = Image.open(os.path.join(IMAGE_DIR, fname)).convert(\"RGB\")\n",
    "            x = preprocess(img).unsqueeze(0).to(device)\n",
    "            # Manually replicate the forward pass to get the features before the\n",
    "            # classification head, as the internal API (like .process_input)\n",
    "            # can change.\n",
    "            # 1. Process input using the private _process_input method\n",
    "            x_processed = vit._process_input(x)\n",
    "            n = x_processed.shape[0]\n",
    "            # 2. Add the class token\n",
    "            batch_class_token = vit.class_token.expand(n, -1, -1)\n",
    "            x_with_token = torch.cat([batch_class_token, x_processed], dim=1)\n",
    "            # 3. Pass through the encoder\n",
    "            encoded_features = vit.encoder(x_with_token)\n",
    "            # 4. Get the class token’s output (this is the feature vector)\n",
    "            features = encoded_features[:, 0]\n",
    "            all_features.append(features.cpu().numpy().flatten())\n",
    "    all_features_np = np.array(all_features)\n",
    "    print(f\"Feature matrix shape: {all_features_np.shape}\")\n",
    "    \n",
    "    # 5. Save embeddings for later use\n",
    "    np.save(EMBEDDINGS_FILE, all_features_np)\n",
    "    print(f\"Embeddings saved to ’{EMBEDDINGS_FILE}’.\")\n",
    "\n",
    "extract_features()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning the data into training/calibration/test data\n",
    "embeddings = np.load(\"embeddings.npy\")\n",
    "\n",
    "SAMPLE_SIZE = 20000\n",
    "\n",
    "TRAIN_RATIO = 0.7\n",
    "CALIBRATION_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "train_index = int(TRAIN_RATIO * SAMPLE_SIZE)\n",
    "calibration_index = train_index + int(CALIBRATION_RATIO * SAMPLE_SIZE)\n",
    "\n",
    "# note - shuffle with labels later\n",
    "train_embed, calibration_embed, test_embed = np.split(embeddings, [train_index, calibration_index]) \n",
    "train_labels = img_df.iloc[:train_index]\n",
    "calibration_labels = img_df.iloc[train_index:calibration_index]\n",
    "test_labels = img_df.iloc[calibration_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d48d13df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood for label Smiling and class 1 is 0.0\n",
      "0.0\n",
      "likelihood for label Smiling and class -1 is 0.0\n",
      "0.0\n",
      "likelihood for label Smiling and class 1 is 0.0\n",
      "likelihood for label Smiling and class -1 is 0.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from numpy import random\n",
    "train_embed_df = pd.DataFrame(train_embed)\n",
    "\n",
    "\n",
    "# We model each feature (conditioned on label) under a gaussian distribution\n",
    "def pd_gaussian(x, mean, sd):  \n",
    "    var = sd**2\n",
    "    exponent = (-(x - mean)**2) / (2 * var)\n",
    "    frac = 1 / (np.sqrt(2 * np.pi * var))\n",
    "    return  frac * np.exp(exponent)\n",
    "\n",
    "\n",
    "def naive_bayes_class(feature, label):\n",
    "    # calc bayes prob for both classes of label\n",
    "    # compare both\n",
    "    print(naive_bayes(feature, label, 1))\n",
    "    print(naive_bayes(feature, label, -1))\n",
    "    classification = 1 if naive_bayes(feature, label, 1) > naive_bayes(feature, label, -1) else 0\n",
    "    return classification\n",
    "\n",
    "\n",
    "def naive_bayes(feature, label, label_class):\n",
    "    # calc prob of feature conditioned on class (pos)\n",
    "    # calc prob of label\n",
    "    return likelihood(feature, label, label_class) * prior(label, label_class)\n",
    "\n",
    "\n",
    "# Returns probability a sample is given a given label/class \n",
    "def prior(label, label_class):\n",
    "    label_count = train_labels[label].value_counts()[label_class]\n",
    "    train_size = SAMPLE_SIZE * TRAIN_RATIO\n",
    "    return (label_count/train_size)\n",
    "\n",
    "\n",
    "# Returns conditional probability a feature is present given a label and class\n",
    "def likelihood(feature, label, label_class):  \n",
    "    likelihood_total = 1\n",
    "\n",
    "    #----may need to move this ----#\n",
    "    means = []\n",
    "    sds = []\n",
    "    temp_df = train_embed_df.assign(label=train_labels[label].values)\n",
    "    feature_class = temp_df[temp_df['label'] == label_class]\n",
    "\n",
    "    for col in feature_class.iloc[:, :-1]:\n",
    "        means.append(feature_class[col].mean())\n",
    "        sds.append(feature_class[col].std())\n",
    "    #------------------------------#\n",
    "\n",
    "    for i in range(len(feature)):\n",
    "        likelihood_total *= pd_gaussian(feature[i], means[i], sds[i])\n",
    "\n",
    "    print(\"likelihood for label \" + str(label) + \" and class \" + str(label_class) + \" is \" + str(likelihood_total))\n",
    "    return likelihood_total\n",
    "    \n",
    "print(naive_bayes_class(embeddings[5], 'Smiling'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72b939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
